---
layout: post
title: "StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks"
byline: "Zhang et al"
arxiv: "1612.03242v2"
tags:
    - GAN
    - text
    - photorealism
    - image-synthesis
    - neural-network
    - computer-vision
    - CV
summary: StackGAN can convert freeform text into an image that matches that description, using two GANs stacked on top of each other in order to best refine details.
---

Generating high quality, high-detail images from scratch is tricky. Generating _specific_ detailed images based on user input is even harder. And generating _specific_, detailed images from freeform text is [an entirely different kind of story altogether](https://youtu.be/0wxp-NxJny8?t=87).

StackGAN is able to convert text like _"This bird is red and brown in color, with a stubby beak"_ into a detailed 256Ã—256 image representation.

GANs are a perfect candidate for this type of complex synthesis, and the authors use two: The first GAN (_Stage-I_) is responsible for "sketching" the output, incorporating the right colors and shapes as per the user's request. The second GAN, _Stage-II_, refines the output of _Stage-I_ in order to improve details and improve shape continuity.

One might wonder why the role of the Stage-II GAN couldn't simply be accomplished by adding more "upscaling" layers to the Stage-I GAN. In comparison with existing systems, the authors demonstrate that this stacked architecture of two GANS, each with a simple job, produces very realistic results, whereas a "deeper" single GAN generally results in unstable, nonsensical outputs where shapes fail to fully resolve.

The researchers demonstrate the StackGAN architecture on the CUB dataset (a ~12K image dataset of 200 bird species) and it dramatically outperforms other models (such as GAN-INT-CLS or GAWWN) in terms of image realism. (In fact, I first though the bottom row of Figure 3 was a ground truth example, not the output of the network. So I guess _this_ discriminator was fooled, at least.)
