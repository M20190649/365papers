---
layout: post
title: "MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment"
byline: "Dong & Hsiao et al"
arxiv: "1709.06298"
tags:
    - music
    - deep-learning
    - GAN
    - machine-learning
    - multi-track
    - polyphony
    - time-series
    - synthesis
    - Lakh-MIDI
summary: MuseGAN uses a variety of models to approximate human-quality musical composition using GAN architectures.
---

My previous papers about music synthesis systems (such as _PiaF_, [#166](http://blog.jordan.matelsky.com/365papers/166), or _JamBot, [#96](http://blog.jordan.matelsky.com/365papers/96)), have mostly relied on machine-learning techniques that approximated existing musical samples or structures.

MuseGAN, and its recent followup, BinaryMuseGAN, leverage recent developments in GAN technology to address the challenges of tempo, beat, and musicality.

The authors propose three models that each generate music-like somethings: A Jamming model, a Composer model, and a Hybrid model. The models were each provided tracks to generate bass, drums, guitar, piano, and string-instrument audio, and were trained on a large rock-music MIDI corpus. The generated samples were graded on harmony, rhythmic structure, multi-track dependency, and temporal structure.

In the Jamming model, many independent agents generate their own interal musical plans. In the Composer model, one single entity generates multiple plans. (The Hybrid model is a combination of the two.)

The results are...mostly what you'd expect. But there are a few tracks that really sound human-generated and very musical. I'd recommend taking a look and a listen.

Data and samples are available [at the project's website](https://salu133445.github.io/musegan/).
