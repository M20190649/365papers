---
layout: post
title: "Fast animal pose estimation using deep neural networks"
byline: Pereira & Aldarondo et al
doi: "10.1101/331181"
tags:
    - neuroscience
    - pose-estimation
    - 3D
    - deep-learning
    - posture
    - postural-dynamics
    - neural-network
    - animal-tracking
summary: This pose-estimation and animal tracking software uses deep learning to estimate the positions and configurations of a freely behaving animal.
---

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">New from the lab!<br><br>Fast and easy pose tracking and kinematics using deep networks, by <a href="https://twitter.com/talmop?ref_src=twsrc%5Etfw">@talmop</a> <a href="https://twitter.com/shaevitz?ref_src=twsrc%5Etfw">@shaevitz</a> <a href="https://twitter.com/SamWangPhD?ref_src=twsrc%5Etfw">@SamWangPhD</a> <a href="https://t.co/KAWimUaW5U">https://t.co/KAWimUaW5U</a> <a href="https://t.co/blXiwL41S7">pic.twitter.com/blXiwL41S7</a></p>&mdash; Adam J Calhoun (@neuroecology) <a href="https://twitter.com/neuroecology/status/1000047514161213442?ref_src=twsrc%5Etfw">May 25, 2018</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


LEAP (LEAP Estimates Animal Poses) is a recursive acronym for a new technology that tracks animals' gaits with very little training data (the paper cites instances in which only a few frames of training are often enough for robust pose estimation).  The input is a 2D video stream and the output is real-space coordinates of user-specified body parts.

LEAP comprises of three steps:

**Registration and alignment**, in which the animal's bounding box is identified and the video feed is redrawn cropped to this area and rotated to face the primary axis of the animal;

**Labeling and training**, in which a user uses a GUI in order to manually label the positions of body parts along a rigid skeleton;

**Pose estimation**, in which the neural network trained in the previous step is used to determine the configuration and orientation of that same skeleton in the frames of the 2D video.

This technology enables far higher fidelity in tracking specific behaviors over time in freely behaving animals. As an example use case, the authors used this technology to analyze the gait dynamics of Drosophila, even on complex backgrounds and with multiple animals in the field of view of the camera.
